---
title: Multileave Gradient Descent for Fast Online Learning to Rank
date: '2016-02-24'
year: 2016
layout: talk
key: multileave-gradient-descent-for-fast-online-learning-2016
redirect_from: /talks/multileave-gradient-descent-for-fast-online-learni-2016.html
shield: conference-orange
venue: WSDM'16
location: San Francisco, USA
slides_url: /assets/20160224-multileleavegradientdescent.pdf
publication_url: /publications/schuth2016multileave
poster_url: /assets/20151124-WSDM-MGD.pdf
---

## Summary

This research addresses the challenge of improving online learning to rank systems by developing Multileave Gradient Descent (MGD), a method that explores multiple ranking directions simultaneously rather than the single direction approach used in traditional Dueling Bandit Gradient Descent (DBGD). The key innovation lies in using multileaved comparisons that can evaluate multiple rankers at once, enabling the algorithm to converge approximately 10 times faster than DBGD while maintaining the same final performance quality. This advancement significantly accelerates the online learning process for search engines, allowing them to adapt more quickly to user feedback and improve ranking performance with fewer update steps.

